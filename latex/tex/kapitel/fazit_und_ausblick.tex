\chapter{Fazit und Ausblick}

% Das erfolgreichste netzwerk wurde mit n5000 into lc0 daten trainiert mit einem lambda von 0.8 (vl etwas ausführlicher erklären das es allein stehen kann) -> mit konkreten daten als vergleich
% Ergebnis, elo im vergleich zu hce
Im Rahmen dieser Arbeit wurde ein \ac{NNUE} Prototyp entwickelt. In einer Reihe von Tests wurden verschiedene Ansätze zur Erstellung eines \acp{NNUE} getestet. Das stärkste Netz ist in zwei Schritten trainiert worden. Zuerst wurde ein Trainingslauf mit Spielen von Stockfish auf niedriger Tiefe ($5000$ Knoten) als Eingabedaten trainiert. Anschließend wurden die Gewichte dieses Trainingslaufs verwendet, um ein Netz mir einem, von \ac{Lc0} generierten, Datensatz zu trainieren. Dabei wurde für die Ermittlung des Fehlers die Kreuzentropie-Verlustfunktion mit einem gewichteten arithmetisches, Mittel von 80 \% Evaluation (in \ac{CP}) und 20 \% Ergebnis des Spiels verwendet. Das Resultierende Netzwerk ist (TODO: elo differenz einfügen) Elo besser als das Basisnetz, welches mit einer Kombination der zwei genannten Datensätze und einem \ac{DFRC} Stockfisch Datensatz und 100 \% \ac{CP} Evaluation, trainiert wurde. Im Vergleich zeigt zu der \ac{HCE} Evaluationsfunktion zeigt es einen Klaren Sieg, mit einer Siegesrate von (TODO: Sieges rate NNUE gegen HCE) und einer Elo Differenz von (TODO: NNUE HCE Elo Differenz). 

% es gibt noch so viele Variablen die man an dem Netz justieren kann, NNUE Archs mit anderer größe oder eine andere cp ratio oder verwendung von bucket/psqt values wie stockfish
Im Training eines \acp{NNUE} gibt es, wie in allen neuronalen Netzen, viele Stellschrauben bzw. Parameter die angepasst werden können. Beispielsweise wurde getestet, ob sich die mittlere quadratische Fehler-Verlustfunktion besser zur Bestimmung des Fehlers eignet, dieser Test schlug fehl. Jedoch gibt es noch viele andere Parameter die angepasst werden können, aber aufgrund der begrenzten Zeit und der Trainingsdauer eines \acp{NNUE} nicht in dieser Arbeit getestet werden konnten. Beispiele für weitere Test ist \zb{} die Verwendung eines anderen Optimierungsalgorithmus, getrennte Gewichte für Weiß und Schwarz oder eine Horizontale Spiegelung statt einer 180-Grad-Rotation des Schachbretts bei der Orientierung für Schwarz.

% empfehlungen wenn man eine eigenes nnue entwickeln möchte
% kleineres feature set
Bei der Entwicklung eines \ac{NNUE} ist zu empfehlen, mit einem kleineren Feature Set als das in dieser Arbeit verwendete HalfKP anzufangen. Die Ergebnisse sind nicht sehr unterschiedlich und es ist leichter verschiedene Anpassungen zu testen.

% ausblick: NNUE ist viel versprechend und es ist noch nicht ausgeschöpft, ebenfalls ineressant ist es anwendungen in der "realität zu finden"
% spekulation: aufgund der hoch effizienten art und weise von NNUE eignet es sich möglicherweise für zeitkritische system mit nur wenigen Änderungen von einem zum nächsten zustand
% ob nnue oder plicy basierte neuronale netze wie von lc0 sich als besser heraustellen oder co-exisiteren ist unklar
\acp{NNUE} sind vielversprechend für die Welt Schachcomputer (bzw. ähnliche Spiele wie Shogi), womöglich gibt es Anwendungsfälle außerhalb dieser Spiele, wie sie AlphaZero mit seinem generellen Ansatz eines Self-Play reinforcement Training basierten \ac{CNN} gefunden hat. Welches der beiden Ansätze sich durchsetzen wird ist nicht klar, aktuell liegt jedoch der \ac{NNUE} Ansatz vorne. Da \ac{NNUE} schnell auf einer CPU funktioniert, liegt es nahe für Anwendungsfälle bei denen keine GPU zur Verfügung steht, wie bei dem durchschnittlichen Nutzer der fall ist.
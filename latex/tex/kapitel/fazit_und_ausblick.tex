\chapter{Fazit und Ausblick}

% Das erfolgreichste netzwerk wurde mit n5000 into lc0 daten trainiert mit einem lambda von 0,8 (vl etwas ausführlicher erklären das es allein stehen kann) -> mit konkreten daten als vergleich
% Ergebnis, elo im vergleich zu hce
Im Rahmen dieser Arbeit wurde ein \ac{NNUE} Prototyp entwickelt. In einer Reihe von Tests wurden verschiedene Ansätze zur Erstellung eines \acp{NNUE} getestet. Das stärkste Netz ist in zwei Schritten trainiert worden. Zuerst wurde ein Trainingslauf mit Spielen von Stockfish auf niedriger Tiefe (5000 Knoten) als Eingabedaten trainiert. Anschließend wurden die Gewichte dieses Trainingslaufs verwendet, um ein Netz mit einem, von \ac{Lc0} generierten Datensatz, zu trainieren. Das resultierende Netzwerk ist 72.5 +- 4.8 Elo besser als das Basisnetz, welches mit einer Kombination der zwei genannten Datensätze und einem \ac{DFRC} Stockfisch Datensatz trainiert wurde. Im Vergleich zu der \ac{HCE} Evaluationsfunktion zeigt es eine klare Verbesserung mit einer Siegesrate von rund 98 \% in \ac{STC} und \ac{LTC} und einer Elo Differenz von 740,9 +- 24,4 in \ac{STC} und 767,0 +- 25,9 in \ac{LTC}. Die Spielstärke verglichen zu anderen Schachcomputern beträgt 2670 +- 6,8 in \ac{STC} und (TODO: Marverick LTC) \ac{LTC} Elo.

% es gibt noch so viele Variablen die man an dem Netz justieren kann, NNUE Archs mit anderer größe oder eine andere cp ratio oder verwendung von bucket/psqt values wie stockfish
Im Training eines \acp{NNUE} gibt es, wie in allen neuronalen Netzen, viele Stellschrauben bzw. Parameter die angepasst werden können. Beispielsweise wurde getestet, ob sich die mittlere quadratische Fehler-Verlustfunktion besser zur Bestimmung des Fehlers eignet. Dieser Test schlug fehl. Jedoch gibt es noch viele andere Parameter, die angepasst werden können, aber aufgrund der begrenzten Zeit und der Trainingsdauer eines \acp{NNUE} nicht in dieser Arbeit getestet werden konnten. Beispiele für weitere Tests sind \zb{} die Verwendung eines anderen Optimierungsalgorithmus, getrennte Gewichte für Weiß und Schwarz oder eine horizontale Spiegelung statt einer 180-Grad-Rotation des Schachbretts bei der Orientierung für Schwarz. Test 7 und 8 sollten als Basis für neue Netze mit einem lambda von 0,8 verwendet werden, damit sie mit den vorherigen Tests vergleichbar sind.

% empfehlungen wenn man eine eigenes nnue entwickeln möchte
% kleineres feature set
Bei der Entwicklung eines \ac{NNUE} ist zu empfehlen, mit einem kleineren Feature Set als das in dieser Arbeit verwendete HalfKP anzufangen. Die Ergebnisse sind nicht sehr unterschiedlich und es ist leichter verschiedene Anpassungen zu testen.

% ausblick: NNUE ist viel versprechend und es ist noch nicht ausgeschöpft, ebenfalls ineressant ist es anwendungen in der "realität zu finden"
% spekulation: aufgund der hoch effizienten art und weise von NNUE eignet es sich möglicherweise für zeitkritische system mit nur wenigen Änderungen von einem zum nächsten zustand
% ob nnue oder plicy basierte neuronale netze wie von lc0 sich als besser heraustellen oder co-exisiteren ist unklar
\acp{NNUE} sind vielversprechend für die Welt Schachcomputer (bzw. ähnliche Spiele wie Shogi). Womöglich gibt es Anwendungsfälle außerhalb dieser Spiele, wie sie AlphaZero mit seinem generellen Ansatz eines Self-Play reinforcement Training basierten \acp{CNN} gefunden hat. Welches der beiden Ansätze sich durchsetzen wird, ist nicht klar. Aktuell liegt jedoch der \ac{NNUE} Ansatz vorne. Für Anwendungsfälle, bei denen keine GPU zur Verfügung steht, liegt es nahe, ein \ac{NNUE} auf der CPU zu verwenden.
\chapter{Diskussion}
\label{chap:discussion}

Die im letzten Kapitel aufgelisteten Ergebnisse zeigen, wie erwartet, einen sehr großen Sieg für \ac{NNUE} gegen \ac{HCE}. Dieses Kapitel analysiert und diskutiert die Ergebnisse. Ziel ist es besser zu verstehen, wie das Training von \acp{NNUE} funktioniert. Außerdem werden Probleme erläutert, die während der Ausarbeitung dieser Arbeit aufgetreten sind.

\section{Erfolge}

In den Tests mit verschiedenen Eingabedaten ist deutlich erkennbar, dass auf einem existierenden Netz basierende Trainingsläufe zu einem Anstieg der Spielstärke führen. Daraus lässt sich schlussfolgern, dass neue Tests aufbauend auf bestehenden starken Netzen trainiert werden sollten. Das bezieht sich auf alle Änderungen, außer Änderungen an der Form des Netzwerks.

Erste Erfolge gegenüber dem Referenz-Netz zeigt Test 3 in \autoref{fig:test5_7}. Interessanterweise ist das Netz mit nur den simplen Stockfish Daten stärker als das Referenznetz mit allen Daten und das Netz, das darauf aufbauend mit allen Daten trainiert wurde, ist nochmal um einiges stärker. Das bedeutet, dass die Komplexität und Reihenfolge einen signifikanten Einfluss auf die Stärke des Netzes haben.

Weiter ist in \autoref{fig:test5_8_9} zu sehen, dass sich durch das Einfügen eines Trainingslaufes mit \ac{Lc0} Daten kein Unterschied in der Spielstärke feststellen lässt. Eine mögliche Erklärung dafür ist, dass die \ac{DFRC} Daten irrelevant sind und bei dem Training mit allen Daten Verbesserungen nur durch die darin enthaltenen \ac{Lc0} Daten erzielt werden. Es ist wahrscheinlich, dass die Inklusion der \ac{DFRC} keinen Einfluss auf die Leistung im normalen Schach hat, aber eine deutliche Verbesserung im \ac{DFRC} Format liefert. Das ist aber für diese Arbeit irrelevant. Somit ist schlusszufolgern, dass der \ac{DFRC} unerheblich ist.

% training mit Lambda 0.8 ist besser, weil falsche Evluationen also Evals, bei denen eine gute Evaluation für ein verlorenes Spiel gegeben wird, weniger stark gewichtet sind.
Die Tests 7 und 8 wurden mit einem Lambda von 0,8 entsprechend der Theorie durchgeführt, dass eine \emph{falsche} Evaluation durch das Spielergebnis ausgeglichen wird. Das heißt, wenn eine Evaluation für einen Vorteil steht, das Spiel jedoch verloren wird, anzunehmen ist, dass die Stellung nicht richtig evaluiert ist. Ganz stimmt das nicht, da nicht sehr tief berechnete Daten verwendet werden (können). So kann in dem Beispiel die Stellung tatsächlich gut sein, aber ein Fehler, der im späteren Spielverlauf auftritt, führt zu einer Niederlage. Da bei der Datengenerierung derselbe Schachcomputer verwendet wird, ist dieser Fall unwahrscheinlich. Deshalb ist davon auszugehen, dass die Inklusion des Spielergebnisses zu einer besseren Evaluation führt, mit dem Vorbehalt, dass diese vermutlich schwerer zu lernen ist, da das Ergebnis in weiter Zukunft liegt. Eigentlich sollte Test 8 ein Retraining basierend auf Test 7 sein, jedoch wurde bei der Initialisierung des Tests nicht Test 7 als Basis definiert. Dieser Fehler zeigt jedoch trotzdem ein überraschendes Ergebnis. Die \ac{Lc0} Daten sorgen für ein besseres Basisnetz. Ein nächster Schritt ist es Test 7 und 8 mit den Daten des jeweils anderen Tests neu zu trainieren. So wird die beste Reihenfolge klar. Außerdem kann das Ergebnis mit dem Resultat aus Test 6 verglichen werden, um zu ermitteln, ob die Verwendung von lambda 0,8 ein stärkeres Netz produziert.

Basierend auf der weiten Verbreitung von \acp{NNUE} in Schachcomputern war davon auszugehen, dass sich der Schachcomputer deutlich gegenüber der \ac{HCE} verbessert. Stockfish hat durch \ac{NNUE} eine Verbesserung von 80 Elo erreicht und Komodo Dragon eine von 197 Elo \cite{StockfishIntroducingNNUE, KomodoDragon}. Es ist zu erwarten, dass der Unterschied bei dem Schachcomputer, der in dieser Arbeit verwendet wird, größer ist, da seine \ac{HCE} Version deutlich simpler ist als Stockfish und Komodo Dragon. Test 5 hat das stärkste Netz produziert und wird somit für den Vergleich zu der \ac{HCE} Version verwendet. Diese Annahme ist mit dem Spiel der \ac{NNUE} Version gegen die \ac{HCE} Version des Schachcomputers bestätigt worden, mit einem Elo gewinn von 740,9 +- 24,4 in \ac{STC} und 767,0 +- 25,9 in \ac{LTC}. Klar ist das die \ac{NNUE} Version der \ac{HCE} Version keine Chance gelassen hat und weniger als 2 \% der verfügbaren Punkte abgegeben hat. Bei einem solch großen Unterschied muss jedoch die ermittelte Elo Wertung kritisch gesehen werden, da bei großen Unterschieden die ermittelte Spielstärke nicht um bedingt übertragbar ist.

Interessant ist es natürlich zu ermitteln wie stark der Schachcomputer im Vergleich zu anderen Schachcomputern ist. Das lässt sich am besten durch ein Turnier gegen einen bereits gewerteten Schachcomputer ermitteln. Dafür wurde ein Schachcomputer mit ähnlicher Stärke aus der \ac{CCRL} \cite{CCRL} Blitz-Schachcomputer-Rangliste ausgewählt. Schachcomputer mit ähnlicher Spielstärke zu wählen ist wichtig, um ein genaues Ergebnis zu erhalten. Im Spiel gegen Maverick 1.5, wurde eine Spielstärke von 2670 +- 6,8 in \ac{STC} und (TODO: Marverick LTC) \ac{LTC} ermittelt. Damit wäre der Schachcomputer unter den Top 100 Spielern weltweit \cite{FIDERating}.

\section{Probleme}

Das gewählte Feature Set HalfKP eignet sich gut für \acp{NNUE}, ist jedoch deutlich größer als ein simples Feature Set mit 768 Merkmalen. Da es sich bei dieser Arbeit lediglich um einen Prototyp handelt, wäre es möglicherweise sinnvoller gewesen mit einem solchen kleineren Netzwerk anzufangen. Das hat den Vorteil, dass es schneller zu trainieren ist und so deutlich mehr Tests zeitlich machbar wären und zu mehr Erfahrungen geführt hätte. Deshalb ist es in der frühen \ac{NNUE} Entwicklungsphase vermutlich besser ein kleineres Feature Set zu wählen. Das zeigt die Einführung von \ac{NNUE} in dem Schachcomputer Smallbrain \cite{Smallbrain}, welcher durch ein kleines \ac{NNUE} mit einer Größe von 768\textrightarrow 256\textrightarrow 1 ähnliche Ergebnisse wie diese Arbeit erhält. Die \ac{HCE} von Smallbrain war vom Aufbau sehr ähnlich zu dem Schachcomputer, der in dieser Arbeit verwendet wurde. Jedoch wurde ein anderer Trainer verwendet und die übrige Struktur der Schachcomputer ist unterschiedlich, somit nicht direkt vergleichbar.

Die Verwendung des Fehlers zu der Bewertung eines Netzes ist unbrauchbar, da der Loss kaum eine Aussage über die Stärke des Netzes liefert. Das Problem ist in \autoref{fig:loss1_2} dargestellt. Die Fehler von Test 1 und 2 sind nicht vergleichbar. Trotz geringem Fehler ist, wie in \autoref{chap:Ergebnisse} zu sehen, die Stärke des Netzes kleiner. Zudem hängt der Fehler von den Eingabedaten ab und eignet sich deshalb nicht für den Vergleich zweier Netzwerke. Deshalb ist die Validierung durch das Durchführen von Turnieren der verschiedenen Netze im Vergleich zu dem aktuell stärksten Netz wichtig. Der Nachteil dabei ist, dass die Durchführung der benötigten Spiele für ein verwertbares Ergebnis hoch ist und somit viel Rechenleistung benötigt. Diese Validierung ist sehr gut. Sie wird auch für Tests in Änderungen außerhalb von \ac{NNUE} verwendet.

\begin{figure}
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        grid=none,
        axis lines=middle,
        xlabel={Epoche},
        ylabel={Fehler},
        ymin=0,
        xmin=0,
        xmax=800,
        scaled y ticks = false,
        yticklabel style={
            /pgf/number format/fixed,
            /pgf/number format/precision=3
          },
        x label style={at={(axis description cs:0.5,-0.1)},anchor=north},
        y label style={at={(axis description cs:-0.15,.5)},rotate=90,anchor=south},
        no marks
      ]
      \addplot table [smooth, x=epoch, y=loss, col sep=comma] {src/loss/run-version_1-tag-training_loss.csv};
      \addlegendentry{Test 1}
      \addplot table [smooth, x=epoch, y=loss, col sep=comma] {src/loss/run-version_2-tag-training_loss.csv};
      \addlegendentry{Test 2}
    \end{axis}
  \end{tikzpicture}
  \caption{Vergleich des Fehlers der Tests 1 und 2. Niedrigerer Fehler bei der Verwendung der mittleren quadratischen Fehler-Verlustfunktion als bei der Kreuzentropie-Verlustfunktion.}
  \label{fig:loss1_2}
\end{figure}

In den Elo-Graphen aller Tests ist, inklusive Abweichung, nicht zu vernachlässigendes Rauschen. Ein gutes Beispiel dafür ist der Test 6, bei welchem in Epoche 775 eine Elo von 85 +- 5,9 und in Epoche 800 eine Elo von 75,9 +- 4,7 ermittelt wurde. Das bedeutet, dass Epoche 775 signifikant besser als Epoche 800 ist. Somit wurde dieser Kontrollpunkt als Repräsentativ für diesen Testlauf gewählt. Jedoch wurde in dieser Arbeit immer der Letzte (Epoche 800) weiter verwendet.

% Zeitkontrolle zu ungenau, spielt mehr nnue oder mehr hce entgegen

% Die Verwendung des \ac{UHO}-Eröffnungsbuches ergibt normalerweise bei dem Computer-gegen-Computer-Vergleich Sinn, aber da die hier getesteten Schachcomputer aufgrund fehlender Tiefe und Tablebase Schwierigkeiten im Endspiel haben, ist es vermutlich von kleinerer Bedeutung, welche Eröffnungen gewählt wurden.

% es wurde für das Training ein Datensatz mit UHO-Eröffnungsbuch gewählt und so auch für das Testen, das kann für ein unrealistisches Ergebnis
Die Verwendung des \ac{UHO}-Eröffnungsbuches stellt nicht den tatsächlichen Aufbau eines Schachspiels dar, da bei einem Spiel aus der Eröffnung kein Computer freiwillig der anderen Seite einen Vorteil in der Eröffnung gibt. Die Alternative, alle Spiele aus der Startposition zu spielen ist jedoch auch nicht sinnvoll, da so alle Spiele sehr ähnlich verlaufen, was die Realität ebenfalls nicht widerspiegelt. Deshalb ist das \ac{UHO}-Eröffnungsbuch eine gute Lösung. So gibt es auch mehr interessante Spiele. Stockfish nutzt ebenfalls \ac{UHO} für das Validieren ihrer Tests \cite{Fishtest}. 

Die Angaben der berechneten Elo durch Self-Play Turniere ist nicht in tatsächliche Elo gegen andere Computer übertragbar, aber sehr gut für den Vergleich verschiedener Versionen eines Schachcomputers. Deshalb wurde für die Ermittlung der Elo im Vergleich zu anderen Schachcomputern ein zufällig ausgewählter, ähnlich starker Schachcomputer aus der \ac{CCRL} \cite{CCRL} Blitz-Schachcomputer-Rangliste ausgewählt.

% Probleme in der Entwicklung:
% mapping from my engine to the uses dateloader was wrong resulting in the input data i gave the net where completly diffrent moves essentaly making the engine blind
% as i was used to in hce i oriented the engine output to the active side, but that already happens by design in the net so essentally it was like the engine playing only for one side giving up all his pieces as white
Das Problem in der Entwicklung des Trainers sowie in der Integration des Interferenz Codes war, dass Fehlerquellen schwer zu identifizieren sind. Gibt es einen Fehler im Interferenz Code oder lernt das Netz schlecht? Auf den Fehler des Trainers kann sich, wie schon besprochen, als Metrik für die Stärke eines Netzes nicht verlassen werden. Die Lösung ist, mit dem Debugger durch den Code zu schreiten und zu hoffen, dass der Fehler gefunden wird, falls es einen gibt. Gemessen an dem Erfolg von \acp{NNUE} ist das wahrscheinlicher als ein schlechtes Netzwerk. Letztendlich war der Fehler in dem Schachcomputer, der eine andere Indizierung der Schachfelder als der Datenlader von Stockfish verwendet. Das hat dafür gesorgt, dass das Netz eine ganz andere Stellung als Eingabevektor erhalten hat als die tatsächliche Stellung und somit die Bewertung für die falsche Stellung ausgegeben wurde.

% vanishing graident untersuchen
% Schichten ausgaben 0 oder 1

% Overfitting ist unwahrscheinlich, da nur ein kleiner Teil der Paramterer gleichzeitig verwendet werden und die hinteren Layer sehr klein sind. Außerdem hilft das sehr große Datenset dagegen, Positionen sollten unterschiedlich genug sein, vor allem da dfrc Data verwendet wird, die noch mehr Diversivität bringt
% weight decay wurde kurz getestet, ist aber nicht sinnvoll, da Overfitting sehr unwahrscheinlich ist
% Dropout-Schichten sind aus dem selben Grund nicht sinvoll, sie versuchen ebenfalls Overfitting zu verhindern 
% kann aber trozdem vorkommen, deshalb wurde das Netz an verschiedenen Zeitpunkten getestet
Ein allgemeines Problem neuronaler Netze ist das sogenannte Overfitting. Das bedeutet, das Netz passt sich zu stark an die Eingabedaten an und ist schlecht in der Einschätzung von Daten außerhalb des Eingabedatensatzes. Das ist für die in dieser Arbeit trainierten Netzwerke kein Problem, da die Anzahl der Trainingsdaten groß genug ist. Außerdem wird immer nur ein kleiner Teil der Parameter gleichzeitig angepasst/verwendet, was ebenfalls Overfitting verhindert. Somit sind Methoden wie \zb{} Weight Decay, Dropout-Schichten nicht nötig.

% Auto-vectorizion could be tested - other engines use it effectively
Das Entwickeln des \ac{SIMD} Quellcodes ist nicht trivial. Es ist elementar sich den Compilierten Assembler Code anzuschauen, da sonst unklar ist, wie der geschriebene Code optimiert ist. Beispielsweise kann das Ausrollen der Schleifen für bessere Performanz sorgen. Wenn sich dafür auf den Compiler verlassen wird, muss das überprüft werden. Oft unterscheiden sich verschiedene Compiler in der Art der Optimierung. Es ist möglich, dass der Compiler verstehen kann, wie der Code automatisch mit \ac{SIMD} vektorisiert werden kann, sicher ist das aber nicht und muss im Assemblercode überprüft werden. Deshalb ist es einfacher, dies selbst mit intrinsischen Funktionen zu entwickeln. Bei der Analyse der verwendeten Stockfish Codes wird schnell klar, dass dieser nicht gut optimiert ist. Das liegt daran, dass der hier verwendete Interferenz Code älter ist, da die verwendete Architektur schon länger nicht in Stockfish verwendet wird. Die neueren Versionen des Stockfish Interferenz Codes sind besser optimiert, aber nicht kompatibel mit HalfKP Netzen ohne Buckets. Das heißt, für eine bessere Performanz muss der hier verwendete Interferenz Code optimiert werden.